{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQ3lAMbizizeOkSsffzdJs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LangChain: Memory\n","\n","## Outline\n","* ConversationBufferMemory\n","* ConversationBufferWindowMemory\n","* ConversationTokenBufferMemory\n","* ConversationSummaryMemory"],"metadata":{"id":"eDmwC1AU9Pz9"}},{"cell_type":"markdown","source":["## Install Required Packages"],"metadata":{"id":"1wuPrz52-ZFc"}},{"cell_type":"code","source":["!pip install -qU langchain langchain-google-genai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-ylBh1y-ap9","executionInfo":{"status":"ok","timestamp":1727509050153,"user_tz":-300,"elapsed":18811,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"75cf212f-60a6-4ebb-94cc-4ed247d40783"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["### Method 01: Using API key for auth"],"metadata":{"id":"79cWGOx_9oDr"}},{"cell_type":"code","source":["# Getting Key from ENVs\n","from google.colab import userdata\n","GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"],"metadata":{"id":"HuS68OUP9TDj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Method 02: Using JSON file for Auth\n","### Complete Procedure can be learn from [here](https://github.com/panaversity/learn-applied-generative-ai-fundamentals/blob/main/21_langchain_ecosystem/langchain/00_gemini_langchain/LangChain_with_Gemini.ipynb), look for **Method 02: Using JSON file for Auth**"],"metadata":{"id":"AyJ4OY9c9sic"}},{"cell_type":"code","source":["# Import the os module to interact with the operating system\n","import os\n","\n","# Set the environment variable for Google Cloud application credentials\n","os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/file.json\"\n"],"metadata":{"id":"QEu9k7Q99v3j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ConversationBufferMemory\n","\n","**ConversationBufferMemory** in LangChain is a tool designed to keep track of the entire conversation history between a human and an AI. Here's a simplified explanation:\n","\n","### What is ConversationBufferMemory?\n","- **Purpose**: It stores the full dialogue history, which helps maintain context in ongoing conversations. This is particularly useful for applications like chatbots or virtual assistants that need to remember previous interactions to provide coherent and contextually relevant responses.\n","\n","### Key Features\n","1. **Storing Messages**:\n","   - It saves both the human's messages and the AI's responses, ensuring that the conversation context is preserved.\n","\n","2. **Retrieving History**:\n","   - The stored conversation can be retrieved in different formats, such as a single string summarizing the entire dialogue or as a list of individual messages. This flexibility allows developers to choose the format that best suits their needs.\n","\n","3. **Integration with Chains**:\n","   - It can be integrated into conversation chains, which are sequences of interactions, to maintain context across multiple exchanges. This ensures that the AI can refer back to previous parts of the conversation, making interactions more natural and fluid.\n","\n","### Practical Use Cases\n","- **Chatbots**: Enhances the chatbot's ability to remember past interactions, making conversations smoother and more personalized.\n","- **Customer Support**: Keeps a record of customer interactions, allowing support agents to provide more informed and effective assistance.\n","- **Personal Assistants**: Helps virtual assistants recall past conversations, improving user experience by making interactions feel more continuous and connected."],"metadata":{"id":"tWUiC9Xd9XO9"}},{"cell_type":"code","source":["from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferMemory\n","\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-1.5-pro\",\n","    api_key=GOOGLE_API_KEY,\n","    temperature=0,\n",")\n","\n","memory = ConversationBufferMemory()\n","\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory=memory,\n","    verbose=True\n",")"],"metadata":{"id":"dMl-8pYm_3gD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conversation.predict(input=\"Hi, my name is Khubaib\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"w3wOrp9ShSBr","executionInfo":{"status":"ok","timestamp":1727509198088,"user_tz":-300,"elapsed":1883,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"041c9a17-e7bd-417c-be14-92a4b2c94375"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: Hi, my name is Khubaib\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Hi Khubaib, it's nice to meet you! I'm an AI, so I don't have a name like you do.  What can I do for you today? 😊 \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["conversation.predict(input=\"What is 1+1?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":292},"id":"tGCp2ymHhR_A","executionInfo":{"status":"ok","timestamp":1727509211391,"user_tz":-300,"elapsed":2318,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"afc6fda4-dd54-45cf-bbc7-0e3ac84888b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: What is 1+1?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"AI: That's an easy one! 1 + 1 equals 2.  Did you know that the concept of addition is one of the earliest mathematical concepts understood by humans?  There's even evidence that ancient civilizations like the Babylonians and Egyptians used addition in their daily lives! 😊 \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["conversation.predict(input=\"What is my name?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"id":"6Z1okbc1hR8Z","executionInfo":{"status":"ok","timestamp":1727509228748,"user_tz":-300,"elapsed":1885,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"342e2f70-5137-4056-c47e-d3f5778eff2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: What is 1+1?\n","AI: AI: That's an easy one! 1 + 1 equals 2.  Did you know that the concept of addition is one of the earliest mathematical concepts understood by humans?  There's even evidence that ancient civilizations like the Babylonians and Egyptians used addition in their daily lives! 😊 \n","\n","Human: What is my name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"AI:  Hmm, that's a tricky one! 🤔 I don't have access to any personal information about you, including your name. To protect your privacy, I'm not able to see or store that kind of data. 😊 \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["print(memory.buffer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wyy1t2D4hR54","executionInfo":{"status":"ok","timestamp":1727509232406,"user_tz":-300,"elapsed":417,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"2fcdfe92-d33f-45fd-b641-3507ef099efe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Human: What is 1+1?\n","AI: AI: That's an easy one! 1 + 1 equals 2.  Did you know that the concept of addition is one of the earliest mathematical concepts understood by humans?  There's even evidence that ancient civilizations like the Babylonians and Egyptians used addition in their daily lives! 😊 \n","\n","Human: What is my name?\n","AI: AI:  Hmm, that's a tricky one! 🤔 I don't have access to any personal information about you, including your name. To protect your privacy, I'm not able to see or store that kind of data. 😊 \n","\n"]}]},{"cell_type":"code","source":["memory.load_memory_variables({})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_byfwaGQhR3L","executionInfo":{"status":"ok","timestamp":1727509236041,"user_tz":-300,"elapsed":418,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"c4cffeca-29bc-4c38-aab1-cb7dae6937f2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"Human: What is 1+1?\\nAI: AI: That's an easy one! 1 + 1 equals 2.  Did you know that the concept of addition is one of the earliest mathematical concepts understood by humans?  There's even evidence that ancient civilizations like the Babylonians and Egyptians used addition in their daily lives! 😊 \\n\\nHuman: What is my name?\\nAI: AI:  Hmm, that's a tricky one! 🤔 I don't have access to any personal information about you, including your name. To protect your privacy, I'm not able to see or store that kind of data. 😊 \\n\"}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["memory = ConversationBufferMemory()"],"metadata":{"id":"ZQTC3q67hRrw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["memory.save_context({\"input\": \"Hi\"},\n","                    {\"output\": \"What's up\"})"],"metadata":{"id":"xBr7tGwehRpT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(memory.buffer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UuztIrMIk0iF","executionInfo":{"status":"ok","timestamp":1727509282064,"user_tz":-300,"elapsed":415,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"aaa3bf12-d33b-4ee0-c61f-17176ace45dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Human: Hi\n","AI: What's up\n"]}]},{"cell_type":"code","source":["memory.load_memory_variables({})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfFVWGbwk0fi","executionInfo":{"status":"ok","timestamp":1727509298511,"user_tz":-300,"elapsed":450,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"3dd718c0-b5e7-4f70-d1e6-eb8fc9e491cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"Human: Hi\\nAI: What's up\"}"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})"],"metadata":{"id":"QicwtoqslAxc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["memory.load_memory_variables({})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LreLYwsjlAXw","executionInfo":{"status":"ok","timestamp":1727509321458,"user_tz":-300,"elapsed":424,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"0f934d26-2bd8-4d2b-bc54-6cd2ba362944"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## ConversationBufferWindowMemory\n","\n","**ConversationBufferWindowMemory** is a memory class in LangChain that manages conversation history by keeping only the most recent interactions. Here's a concise explanation:\n","\n","### What is ConversationBufferWindowMemory?\n","- **Purpose**: It stores a limited number of the most recent messages in a conversation, rather than the entire history. This helps in maintaining context without overwhelming the system with too much data.\n","\n","### Key Features\n","1. **Sliding Window of Interactions**:\n","   - It keeps a \"sliding window\" of the last \\( K \\) interactions. This means only the most recent \\( K \\) messages are stored, and older messages are discarded. This is useful for applications where only recent context is needed.\n","\n","2. **Efficient Memory Management**:\n","   - By limiting the number of stored messages, it prevents the memory buffer from becoming too large, which can be beneficial for performance and resource management.\n","\n","3. **Retrieving Recent History**:\n","   - The recent conversation history can be retrieved in different formats, such as a single string summarizing the last few interactions or as a list of individual messages. This flexibility allows developers to choose the format that best suits their needs.\n","\n","### Practical Use Cases\n","- **Chatbots**: Ensures the chatbot remembers the most recent part of the conversation, making interactions more relevant without storing unnecessary older messages.\n","- **Customer Support**: Helps support agents focus on the latest issues discussed with a customer, improving response efficiency.\n","- **Personal Assistants**: Allows virtual assistants to recall recent interactions, enhancing user experience by maintaining relevant context."],"metadata":{"id":"KYWKhmeNlIgk"}},{"cell_type":"code","source":["from langchain.memory import ConversationBufferWindowMemory"],"metadata":{"id":"bs1n8NFLlAUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["memory = ConversationBufferWindowMemory(k=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ooylCfgDlQ21","executionInfo":{"status":"ok","timestamp":1727509381525,"user_tz":-300,"elapsed":486,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"c469d511-e5aa-4d01-dcd4-629b458a0e69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-17-edd6ddba5fe5>:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationBufferWindowMemory(k=1)\n"]}]},{"cell_type":"code","source":["memory.save_context({\"input\": \"Hi\"},\n","                    {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})\n"],"metadata":{"id":"6aTKpgfclQuN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["memory.load_memory_variables({})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFC48Sf0lQi9","executionInfo":{"status":"ok","timestamp":1727509404699,"user_tz":-300,"elapsed":425,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"5b83f0d8-f448-46f1-efbd-ff8186d1cd9b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'Human: Not much, just hanging\\nAI: Cool'}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["memory = ConversationBufferWindowMemory(k=1)\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=False\n",")"],"metadata":{"id":"0g_Q-SPBk0cx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conversation.predict(input=\"Hi, my name is Andrew\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"oV8EONcjk0Zv","executionInfo":{"status":"ok","timestamp":1727509437842,"user_tz":-300,"elapsed":1914,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"37a9fcc4-7d65-495f-be48-6b25896be6d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Hi Andrew, it's nice to meet you! I'm an AI, so I don't have a name like you do.  What can I do for you today? 😊 \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["conversation.predict(input=\"What is 1+1?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"JTYCW3Hqk0W6","executionInfo":{"status":"ok","timestamp":1727509459819,"user_tz":-300,"elapsed":2081,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"2babc03d-2892-4b29-e570-f1d709a69523"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"AI: That's an easy one! 1+1 equals 2. Did you have any other math problems you needed help with?  I'm pretty good with numbers! 😉 \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["conversation.predict(input=\"What is my name?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"zlOTgwoEk0UK","executionInfo":{"status":"ok","timestamp":1727509478167,"user_tz":-300,"elapsed":2194,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"cd91e61d-7368-41ba-bb82-636ebd76a7ab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"AI:  Hmm, that's a tricky one! I don't have access to any information about you, like your name.  To know that, I'd need you to tell me! 😊  Would you like to share your name with me? \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["## ConversationTokenBufferMemory\n","\n","**ConversationTokenBufferMemory** is a memory class in LangChain that manages conversation history by limiting the stored content based on the number of tokens rather than the number of interactions. Here's a detailed explanation:\n","\n","### What is ConversationTokenBufferMemory?\n","- **Purpose**: It stores recent interactions in a conversation, but instead of counting the number of messages, it uses the total number of tokens (words or characters) to determine when to discard older messages. This helps in maintaining context while managing memory efficiently.\n","\n","### Key Features\n","1. **Token-Based Limit**:\n","   - It keeps a buffer of recent interactions and uses a token limit to decide when to flush older messages. This ensures that the memory buffer does not exceed a specified token count, which can be crucial for performance and cost management, especially when using large language models (LLMs).\n","\n","2. **Efficient Memory Management**:\n","   - By focusing on token count, it allows for more granular control over memory usage. This is particularly useful for applications where the length of individual messages can vary significantly.\n","\n","3. **Retrieving Recent History**:\n","   - The recent conversation history can be retrieved in different formats, such as a single string summarizing the last few interactions or as a list of individual messages. This flexibility allows developers to choose the format that best suits their needs.\n","\n","### Practical Use Cases\n","- **Chatbots**: Ensures the chatbot remembers the most relevant part of the conversation without storing unnecessary older messages, making interactions more efficient.\n","- **Customer Support**: Helps support agents focus on the latest issues discussed with a customer, improving response efficiency while managing memory usage.\n","- **Personal Assistants**: Allows virtual assistants to recall recent interactions, enhancing user experience by maintaining relevant context without overwhelming the system."],"metadata":{"id":"yed5f2Qypfkm"}},{"cell_type":"code","source":["!pip install -qU tiktoken"],"metadata":{"id":"M-oi4978k0Eb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.memory import ConversationTokenBufferMemory"],"metadata":{"id":"rS8onvKzk0CJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n","memory.save_context({\"input\": \"AI is what?!\"},\n","                    {\"output\": \"Amazing!\"})\n","memory.save_context({\"input\": \"Backpropagation is what?\"},\n","                    {\"output\": \"Beautiful!\"})\n","memory.save_context({\"input\": \"Chatbots are what?\"},\n","                    {\"output\": \"Charming!\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9hMBxjZkz_4","executionInfo":{"status":"ok","timestamp":1727510604793,"user_tz":-300,"elapsed":1764,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"e04463dd-e526-4663-95eb-95c03032126e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-28-864c49f39089>:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n"]}]},{"cell_type":"code","source":["memory.load_memory_variables({})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4J9UutECqrWd","executionInfo":{"status":"ok","timestamp":1727510790035,"user_tz":-300,"elapsed":465,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"048c0f10-5aaf-40de-8cf6-d3c374c1ba99"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## ConversationSummaryMemory\n","\n","**ConversationSummaryMemory** is a memory class in LangChain designed to manage conversation history by creating a summary of the dialogue over time. Here's a detailed explanation:\n","\n","### What is ConversationSummaryMemory?\n","- **Purpose**: It summarizes the conversation as it progresses, storing a condensed version of the dialogue. This helps maintain context without keeping the entire conversation history, which can be beneficial for long-term interactions.\n","\n","### Key Features\n","1. **Progressive Summarization**:\n","   - As the conversation continues, it generates and updates a summary that encapsulates the main points and context of the dialogue. This summary is continuously refined to include new information while keeping the overall context concise.\n","\n","2. **Efficient Memory Management**:\n","   - By summarizing the conversation, it reduces the number of tokens needed to store the dialogue history. This is particularly useful for applications where token limits are a concern, such as when using large language models (LLMs).\n","\n","3. **Retrieving Summarized History**:\n","   - The summarized conversation can be retrieved and used to provide context for future interactions. This ensures that the AI can refer back to the essential points of the conversation without needing to process the entire history.\n","\n","### Practical Use Cases\n","- **Chatbots**: Enhances the chatbot's ability to maintain context over long conversations without storing every single message, making interactions more efficient.\n","- **Customer Support**: Helps support agents quickly understand the main issues discussed with a customer, improving response times and effectiveness.\n","- **Personal Assistants**: Allows virtual assistants to recall the key points of past interactions, enhancing user experience by maintaining relevant context."],"metadata":{"id":"N7d2fMJEq00_"}},{"cell_type":"code","source":["from langchain.memory import ConversationSummaryBufferMemory"],"metadata":{"id":"y0iEzvBdkz9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a long string\n","schedule = \"There is a meeting at 8am with your product team. \\\n","You will need your powerpoint presentation prepared. \\\n","9am-12pm have time to work on your LangChain \\\n","project which will go quickly because Langchain is such a powerful tool. \\\n","At Noon, lunch at the italian resturant with a customer who is driving \\\n","from over an hour away to meet you to understand the latest in AI. \\\n","Be sure to bring your laptop to show the latest LLM demo.\"\n","\n","memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n","memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})\n","memory.save_context({\"input\": \"What is on the schedule today?\"},\n","                    {\"output\": f\"{schedule}\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0m-dNH3kz5D","executionInfo":{"status":"ok","timestamp":1727510847915,"user_tz":-300,"elapsed":3401,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"a23dc0a2-f4d4-44e9-ab5c-15a4ecf45791"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-31-c93b773668f2>:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"]}]},{"cell_type":"code","source":["memory.load_memory_variables({})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pqc_HYkAkzxj","executionInfo":{"status":"ok","timestamp":1727510858183,"user_tz":-300,"elapsed":416,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"78590d6d-fcbf-4b50-dc33-dc605db3b2a3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"System: New summary:\\nThe human greets the AI. The AI responds and asks what the human is doing. The human is relaxing, and the AI acknowledges. The human then inquires about the day's schedule. \\n\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"],"metadata":{"id":"DRE5K9vTkzvM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conversation.predict(input=\"What would be a good demo to show?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"id":"qtxNUEHSkzsW","executionInfo":{"status":"ok","timestamp":1727510884852,"user_tz":-300,"elapsed":6574,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"8b7b5203-070c-460e-c877-096742b7ee7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","System: New summary:\n","The human greets the AI. The AI responds and asks what the human is doing. The human is relaxing, and the AI acknowledges. The human then inquires about the day's schedule. \n","\n","AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n","Human: What would be a good demo to show?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"AI: A great demo would be to showcase how LangChain can be used to build a customer service chatbot tailored to your client's industry. You could pre-load it with some of their company data, like product FAQs or recent press releases, to show how it can provide accurate and relevant information to customer inquiries.  It's interactive, highlights the practical applications of AI, and allows you to tailor the conversation to their specific needs and interests. \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["memory.load_memory_variables({})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpw385ohkzpo","executionInfo":{"status":"ok","timestamp":1727510889503,"user_tz":-300,"elapsed":447,"user":{"displayName":"Khubaib khan","userId":"10178393777184038281"}},"outputId":"35e122c8-23cf-49db-d56d-ba70416d2599"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"System: The human greets the AI. The AI responds and asks what the human is doing. The human is relaxing, and the AI acknowledges. The human then inquires about the day's schedule. The AI provides the schedule: an 8 am meeting with the product team requiring a PowerPoint presentation, followed by time to work on the LangChain project from 9 am to 12 pm. At noon, there's a lunch meeting at an Italian restaurant with a customer interested in the latest AI advancements, requiring the human to bring their laptop for an LLM demo. The human then asks for suggestions on a good demo to show the customer. \\n\\nAI: AI: A great demo would be to showcase how LangChain can be used to build a customer service chatbot tailored to your client's industry. You could pre-load it with some of their company data, like product FAQs or recent press releases, to show how it can provide accurate and relevant information to customer inquiries.  It's interactive, highlights the practical applications of AI, and allows you to tailor the conversation to their specific needs and interests. \\n\"}"]},"metadata":{},"execution_count":35}]}]}